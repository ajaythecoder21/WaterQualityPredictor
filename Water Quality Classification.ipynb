{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Quality Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('water_potability.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>204.890455</td>\n",
       "      <td>20791.318981</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>129.422921</td>\n",
       "      <td>18630.057858</td>\n",
       "      <td>6.635246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>15.180013</td>\n",
       "      <td>56.329076</td>\n",
       "      <td>4.500656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>224.236259</td>\n",
       "      <td>19909.541732</td>\n",
       "      <td>9.275884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>16.868637</td>\n",
       "      <td>66.420093</td>\n",
       "      <td>3.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.417441</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436524</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.986339</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
       "0       NaN  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
       "1  3.716080  129.422921  18630.057858     6.635246         NaN    592.885359   \n",
       "2  8.099124  224.236259  19909.541732     9.275884         NaN    418.606213   \n",
       "3  8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
       "4  9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
       "\n",
       "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0       10.379783        86.990970   2.963135           0  \n",
       "1       15.180013        56.329076   4.500656           0  \n",
       "2       16.868637        66.420093   3.055934           0  \n",
       "3       18.436524       100.341674   4.628771           0  \n",
       "4       11.558279        31.997993   4.075075           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2785 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          2495 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3114 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2785.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>2495.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3114.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.080795</td>\n",
       "      <td>196.369496</td>\n",
       "      <td>22014.092526</td>\n",
       "      <td>7.122277</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>426.205111</td>\n",
       "      <td>14.284970</td>\n",
       "      <td>66.396293</td>\n",
       "      <td>3.966786</td>\n",
       "      <td>0.390110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.594320</td>\n",
       "      <td>32.879761</td>\n",
       "      <td>8768.570828</td>\n",
       "      <td>1.583085</td>\n",
       "      <td>41.416840</td>\n",
       "      <td>80.824064</td>\n",
       "      <td>3.308162</td>\n",
       "      <td>16.175008</td>\n",
       "      <td>0.780382</td>\n",
       "      <td>0.487849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.432000</td>\n",
       "      <td>320.942611</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>181.483754</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.093092</td>\n",
       "      <td>176.850538</td>\n",
       "      <td>15666.690297</td>\n",
       "      <td>6.127421</td>\n",
       "      <td>307.699498</td>\n",
       "      <td>365.734414</td>\n",
       "      <td>12.065801</td>\n",
       "      <td>55.844536</td>\n",
       "      <td>3.439711</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.036752</td>\n",
       "      <td>196.967627</td>\n",
       "      <td>20927.833607</td>\n",
       "      <td>7.130299</td>\n",
       "      <td>333.073546</td>\n",
       "      <td>421.884968</td>\n",
       "      <td>14.218338</td>\n",
       "      <td>66.622485</td>\n",
       "      <td>3.955028</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.062066</td>\n",
       "      <td>216.667456</td>\n",
       "      <td>27332.762127</td>\n",
       "      <td>8.114887</td>\n",
       "      <td>359.950170</td>\n",
       "      <td>481.792304</td>\n",
       "      <td>16.557652</td>\n",
       "      <td>77.337473</td>\n",
       "      <td>4.500320</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>323.124000</td>\n",
       "      <td>61227.196008</td>\n",
       "      <td>13.127000</td>\n",
       "      <td>481.030642</td>\n",
       "      <td>753.342620</td>\n",
       "      <td>28.300000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>6.739000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ph     Hardness        Solids  Chloramines      Sulfate  \\\n",
       "count  2785.000000  3276.000000   3276.000000  3276.000000  2495.000000   \n",
       "mean      7.080795   196.369496  22014.092526     7.122277   333.775777   \n",
       "std       1.594320    32.879761   8768.570828     1.583085    41.416840   \n",
       "min       0.000000    47.432000    320.942611     0.352000   129.000000   \n",
       "25%       6.093092   176.850538  15666.690297     6.127421   307.699498   \n",
       "50%       7.036752   196.967627  20927.833607     7.130299   333.073546   \n",
       "75%       8.062066   216.667456  27332.762127     8.114887   359.950170   \n",
       "max      14.000000   323.124000  61227.196008    13.127000   481.030642   \n",
       "\n",
       "       Conductivity  Organic_carbon  Trihalomethanes    Turbidity   Potability  \n",
       "count   3276.000000     3276.000000      3114.000000  3276.000000  3276.000000  \n",
       "mean     426.205111       14.284970        66.396293     3.966786     0.390110  \n",
       "std       80.824064        3.308162        16.175008     0.780382     0.487849  \n",
       "min      181.483754        2.200000         0.738000     1.450000     0.000000  \n",
       "25%      365.734414       12.065801        55.844536     3.439711     0.000000  \n",
       "50%      421.884968       14.218338        66.622485     3.955028     0.000000  \n",
       "75%      481.792304       16.557652        77.337473     4.500320     1.000000  \n",
       "max      753.342620       28.300000       124.000000     6.739000     1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ph_null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ph                 491\n",
       "Hardness             0\n",
       "Solids               0\n",
       "Chloramines          0\n",
       "Sulfate            781\n",
       "Conductivity         0\n",
       "Organic_carbon       0\n",
       "Trihalomethanes    162\n",
       "Turbidity            0\n",
       "Potability           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>204.890455</td>\n",
       "      <td>20791.318981</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>118.988579</td>\n",
       "      <td>14285.583854</td>\n",
       "      <td>7.804174</td>\n",
       "      <td>268.646941</td>\n",
       "      <td>389.375566</td>\n",
       "      <td>12.706049</td>\n",
       "      <td>53.928846</td>\n",
       "      <td>3.595017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>150.174923</td>\n",
       "      <td>27331.361962</td>\n",
       "      <td>6.838223</td>\n",
       "      <td>299.415781</td>\n",
       "      <td>379.761835</td>\n",
       "      <td>19.370807</td>\n",
       "      <td>76.509996</td>\n",
       "      <td>4.413974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>227.435048</td>\n",
       "      <td>22305.567414</td>\n",
       "      <td>10.333918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>554.820086</td>\n",
       "      <td>16.331693</td>\n",
       "      <td>45.382815</td>\n",
       "      <td>4.133423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>215.977859</td>\n",
       "      <td>17107.224226</td>\n",
       "      <td>5.607060</td>\n",
       "      <td>326.943978</td>\n",
       "      <td>436.256194</td>\n",
       "      <td>14.189062</td>\n",
       "      <td>59.855476</td>\n",
       "      <td>5.459251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>NaN</td>\n",
       "      <td>198.218700</td>\n",
       "      <td>31081.735264</td>\n",
       "      <td>7.419106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>517.925946</td>\n",
       "      <td>11.711419</td>\n",
       "      <td>85.428785</td>\n",
       "      <td>3.345543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>NaN</td>\n",
       "      <td>203.204659</td>\n",
       "      <td>10643.186771</td>\n",
       "      <td>6.828936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>384.597711</td>\n",
       "      <td>16.011328</td>\n",
       "      <td>72.911573</td>\n",
       "      <td>3.065910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231</th>\n",
       "      <td>NaN</td>\n",
       "      <td>225.754109</td>\n",
       "      <td>28194.452646</td>\n",
       "      <td>5.892830</td>\n",
       "      <td>366.201583</td>\n",
       "      <td>418.272901</td>\n",
       "      <td>17.306832</td>\n",
       "      <td>103.912548</td>\n",
       "      <td>3.855895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>NaN</td>\n",
       "      <td>188.536608</td>\n",
       "      <td>24711.414927</td>\n",
       "      <td>7.129520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>555.548534</td>\n",
       "      <td>16.959269</td>\n",
       "      <td>56.038702</td>\n",
       "      <td>4.331691</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>NaN</td>\n",
       "      <td>134.736856</td>\n",
       "      <td>9000.025591</td>\n",
       "      <td>9.026293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>428.213987</td>\n",
       "      <td>8.668672</td>\n",
       "      <td>74.773392</td>\n",
       "      <td>3.699558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
       "0    NaN  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
       "8    NaN  118.988579  14285.583854     7.804174  268.646941    389.375566   \n",
       "13   NaN  150.174923  27331.361962     6.838223  299.415781    379.761835   \n",
       "20   NaN  227.435048  22305.567414    10.333918         NaN    554.820086   \n",
       "22   NaN  215.977859  17107.224226     5.607060  326.943978    436.256194   \n",
       "...   ..         ...           ...          ...         ...           ...   \n",
       "3224 NaN  198.218700  31081.735264     7.419106         NaN    517.925946   \n",
       "3229 NaN  203.204659  10643.186771     6.828936         NaN    384.597711   \n",
       "3231 NaN  225.754109  28194.452646     5.892830  366.201583    418.272901   \n",
       "3245 NaN  188.536608  24711.414927     7.129520         NaN    555.548534   \n",
       "3260 NaN  134.736856   9000.025591     9.026293         NaN    428.213987   \n",
       "\n",
       "      Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0          10.379783        86.990970   2.963135           0  \n",
       "8          12.706049        53.928846   3.595017           0  \n",
       "13         19.370807        76.509996   4.413974           0  \n",
       "20         16.331693        45.382815   4.133423           0  \n",
       "22         14.189062        59.855476   5.459251           0  \n",
       "...              ...              ...        ...         ...  \n",
       "3224       11.711419        85.428785   3.345543           1  \n",
       "3229       16.011328        72.911573   3.065910           1  \n",
       "3231       17.306832       103.912548   3.855895           1  \n",
       "3245       16.959269        56.038702   4.331691           1  \n",
       "3260        8.668672        74.773392   3.699558           1  \n",
       "\n",
       "[491 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['ph'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['pH_null'] = np.where(df['ph'].isnull(), 1, 0) # if null, replace 1 or 0\n",
    "df['sulfate_null'] = np.where(df['Sulfate'].isnull(), 1, 0)\n",
    "df['tri_null'] = np.where(df['Trihalomethanes'].isnull(), 1, 0)\n",
    "# find percentage of null values\n",
    "df['ph'].fillna(df['ph'].median(), inplace=True)\n",
    "df['Sulfate'].fillna(df['Sulfate'].median(), inplace=True)\n",
    "df['Trihalomethanes'].fillna(df['Trihalomethanes'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2a34dd65550>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['ph'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras_tuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 20)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                         min_value=32,\n",
    "                         max_value=512,\n",
    "                         step=50),\n",
    "                  activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "        hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project project2\\Simple Tuning\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from project2\\Simple Tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='project2',\n",
    "    project_name='Simple Tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 17\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 20, 'step': 1, 'sampling': None}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_4 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_5 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_6 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_7 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_8 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_9 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_10 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_11 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_12 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_13 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n",
      "units_14 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 50, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=5,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in project2\\Simple Tuning\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 12\n",
      "units_0: 482\n",
      "units_1: 82\n",
      "learning_rate: 0.001\n",
      "units_2: 82\n",
      "units_3: 382\n",
      "units_4: 132\n",
      "units_5: 332\n",
      "units_6: 182\n",
      "units_7: 232\n",
      "units_8: 482\n",
      "units_9: 232\n",
      "units_10: 432\n",
      "units_11: 132\n",
      "units_12: 332\n",
      "units_13: 482\n",
      "units_14: 382\n",
      "Score: 0.950152575969696\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 15\n",
      "units_0: 332\n",
      "units_1: 82\n",
      "learning_rate: 0.0001\n",
      "units_2: 432\n",
      "units_3: 232\n",
      "units_4: 182\n",
      "units_5: 32\n",
      "units_6: 32\n",
      "units_7: 32\n",
      "units_8: 32\n",
      "units_9: 32\n",
      "units_10: 32\n",
      "units_11: 32\n",
      "units_12: 32\n",
      "units_13: 32\n",
      "units_14: 32\n",
      "Score: 0.950152575969696\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 8\n",
      "units_0: 332\n",
      "units_1: 332\n",
      "learning_rate: 0.0001\n",
      "units_2: 382\n",
      "units_3: 182\n",
      "units_4: 382\n",
      "units_5: 282\n",
      "units_6: 332\n",
      "units_7: 132\n",
      "units_8: 482\n",
      "units_9: 332\n",
      "units_10: 332\n",
      "units_11: 382\n",
      "units_12: 432\n",
      "units_13: 82\n",
      "units_14: 132\n",
      "Score: 0.950152575969696\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "units_0: 282\n",
      "units_1: 232\n",
      "learning_rate: 0.001\n",
      "units_2: 332\n",
      "units_3: 82\n",
      "units_4: 82\n",
      "units_5: 282\n",
      "units_6: 332\n",
      "units_7: 332\n",
      "units_8: 282\n",
      "units_9: 382\n",
      "units_10: 482\n",
      "units_11: 182\n",
      "units_12: 482\n",
      "units_13: 382\n",
      "units_14: 182\n",
      "Score: 0.3499491289258003\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "units_0: 382\n",
      "units_1: 232\n",
      "learning_rate: 0.001\n",
      "units_2: 32\n",
      "units_3: 32\n",
      "units_4: 32\n",
      "Score: 0.3499491289258003\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(332, activation='relu'))\n",
    "model.add(layers.Dense(332, activation='relu'))\n",
    "model.add(layers.Dense(382, activation='relu'))\n",
    "model.add(layers.Dense(182, activation='relu'))\n",
    "model.add(layers.Dense(382, activation='relu'))\n",
    "model.add(layers.Dense(282, activation='relu'))\n",
    "model.add(layers.Dense(332, activation='relu'))\n",
    "model.add(layers.Dense(132, activation='relu'))\n",
    "model.add(layers.Dense(482, activation='relu'))\n",
    "model.add(layers.Dense(332, activation='relu'))\n",
    "model.add(layers.Dense(332, activation='relu'))\n",
    "model.add(layers.Dense(382, activation='relu'))\n",
    "model.add(layers.Dense(432, activation='relu'))\n",
    "model.add(layers.Dense(82, activation='relu'))\n",
    "model.add(layers.Dense(132, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(keras.optimizers.Adam(learning_rate=1e-4), keras.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "WARNING:tensorflow:Layer dense_36 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4252 - accuracy: 0.9285\n",
      "Epoch 2/250\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.2610 - accuracy: 0.9507\n",
      "Epoch 3/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2516 - accuracy: 0.9507: 0s - loss: 0.252\n",
      "Epoch 4/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2331 - accuracy: 0.9507\n",
      "Epoch 5/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.2380 - accuracy: 0.9507\n",
      "Epoch 6/250\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.2391 - accuracy: 0.9507\n",
      "Epoch 7/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2330 - accuracy: 0.9507\n",
      "Epoch 8/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2288 - accuracy: 0.9507\n",
      "Epoch 9/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2320 - accuracy: 0.9507\n",
      "Epoch 10/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2318 - accuracy: 0.9507\n",
      "Epoch 11/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2181 - accuracy: 0.9507\n",
      "Epoch 12/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.2382 - accuracy: 0.9507\n",
      "Epoch 13/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2346 - accuracy: 0.9507\n",
      "Epoch 14/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2212 - accuracy: 0.9507\n",
      "Epoch 15/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2205 - accuracy: 0.9507\n",
      "Epoch 16/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2182 - accuracy: 0.9507\n",
      "Epoch 17/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2217 - accuracy: 0.9507\n",
      "Epoch 18/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2147 - accuracy: 0.9507\n",
      "Epoch 19/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2140 - accuracy: 0.9507\n",
      "Epoch 20/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.2095 - accuracy: 0.9507\n",
      "Epoch 21/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2100 - accuracy: 0.9507\n",
      "Epoch 22/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2260 - accuracy: 0.9507\n",
      "Epoch 23/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.2165 - accuracy: 0.9507\n",
      "Epoch 24/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2101 - accuracy: 0.9507\n",
      "Epoch 25/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2113 - accuracy: 0.9507\n",
      "Epoch 26/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2077 - accuracy: 0.9507\n",
      "Epoch 27/250\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.2097 - accuracy: 0.9507\n",
      "Epoch 28/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2059 - accuracy: 0.9507\n",
      "Epoch 29/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2071 - accuracy: 0.9507\n",
      "Epoch 30/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2015 - accuracy: 0.9507\n",
      "Epoch 31/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2042 - accuracy: 0.9507\n",
      "Epoch 32/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2037 - accuracy: 0.9507\n",
      "Epoch 33/250\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.2141 - accuracy: 0.9507\n",
      "Epoch 34/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2138 - accuracy: 0.9507\n",
      "Epoch 35/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.2059 - accuracy: 0.9507\n",
      "Epoch 36/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2109 - accuracy: 0.9507\n",
      "Epoch 37/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2070 - accuracy: 0.9507\n",
      "Epoch 38/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2115 - accuracy: 0.9507\n",
      "Epoch 39/250\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 0.2119 - accuracy: 0.9507\n",
      "Epoch 40/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2095 - accuracy: 0.9507\n",
      "Epoch 41/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2134 - accuracy: 0.9507\n",
      "Epoch 42/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2036 - accuracy: 0.9507\n",
      "Epoch 43/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2104 - accuracy: 0.9507\n",
      "Epoch 44/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2082 - accuracy: 0.9507\n",
      "Epoch 45/250\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.2027 - accuracy: 0.9507\n",
      "Epoch 46/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2096 - accuracy: 0.9507\n",
      "Epoch 47/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2054 - accuracy: 0.9507\n",
      "Epoch 48/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2024 - accuracy: 0.9507\n",
      "Epoch 49/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2003 - accuracy: 0.9507\n",
      "Epoch 50/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.2013 - accuracy: 0.9507\n",
      "Epoch 51/250\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.2045 - accuracy: 0.9507\n",
      "Epoch 52/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2050 - accuracy: 0.9507\n",
      "Epoch 53/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2021 - accuracy: 0.9507\n",
      "Epoch 54/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2074 - accuracy: 0.9507\n",
      "Epoch 55/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2105 - accuracy: 0.9507\n",
      "Epoch 56/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.2022 - accuracy: 0.9507\n",
      "Epoch 57/250\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.2074 - accuracy: 0.9507\n",
      "Epoch 58/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2142 - accuracy: 0.9507\n",
      "Epoch 59/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2027 - accuracy: 0.9507\n",
      "Epoch 60/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2049 - accuracy: 0.9507\n",
      "Epoch 61/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.2025 - accuracy: 0.9507\n",
      "Epoch 62/250\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.2025 - accuracy: 0.9507\n",
      "Epoch 63/250\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.2036 - accuracy: 0.9507\n",
      "Epoch 64/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2087 - accuracy: 0.9507 0s - loss: 0.2136 - accu\n",
      "Epoch 65/250\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.2205 - accuracy: 0.9507\n",
      "Epoch 66/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.2015 - accuracy: 0.9507\n",
      "Epoch 67/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2026 - accuracy: 0.9507\n",
      "Epoch 68/250\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.2018 - accuracy: 0.9507\n",
      "Epoch 69/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1996 - accuracy: 0.9507 0s - loss: 0.1874 - accu\n",
      "Epoch 70/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2023 - accuracy: 0.9507\n",
      "Epoch 71/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.2060 - accuracy: 0.9507 0s - loss: 0.2097 - ac\n",
      "Epoch 72/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2060 - accuracy: 0.9507\n",
      "Epoch 73/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2019 - accuracy: 0.9507\n",
      "Epoch 74/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 1s 17ms/step - loss: 0.2055 - accuracy: 0.9507\n",
      "Epoch 75/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2010 - accuracy: 0.9507\n",
      "Epoch 76/250\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.2054 - accuracy: 0.9507\n",
      "Epoch 77/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2059 - accuracy: 0.9507\n",
      "Epoch 78/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2033 - accuracy: 0.9507\n",
      "Epoch 79/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1997 - accuracy: 0.9507\n",
      "Epoch 80/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2020 - accuracy: 0.9507\n",
      "Epoch 81/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2023 - accuracy: 0.9507\n",
      "Epoch 82/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2011 - accuracy: 0.9507\n",
      "Epoch 83/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2030 - accuracy: 0.9507\n",
      "Epoch 84/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2029 - accuracy: 0.9507\n",
      "Epoch 85/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2017 - accuracy: 0.9507\n",
      "Epoch 86/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.2041 - accuracy: 0.9507\n",
      "Epoch 87/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1995 - accuracy: 0.9507\n",
      "Epoch 88/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.2060 - accuracy: 0.9507\n",
      "Epoch 89/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2037 - accuracy: 0.9507\n",
      "Epoch 90/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2003 - accuracy: 0.9507\n",
      "Epoch 91/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2040 - accuracy: 0.9507\n",
      "Epoch 92/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1976 - accuracy: 0.9507\n",
      "Epoch 93/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1992 - accuracy: 0.9507\n",
      "Epoch 94/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2020 - accuracy: 0.9507\n",
      "Epoch 95/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2000 - accuracy: 0.9507\n",
      "Epoch 96/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2017 - accuracy: 0.9507\n",
      "Epoch 97/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1995 - accuracy: 0.9507\n",
      "Epoch 98/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1966 - accuracy: 0.9507\n",
      "Epoch 99/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.2045 - accuracy: 0.9507\n",
      "Epoch 100/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2008 - accuracy: 0.9507\n",
      "Epoch 101/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1998 - accuracy: 0.9507\n",
      "Epoch 102/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1964 - accuracy: 0.9507\n",
      "Epoch 103/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2032 - accuracy: 0.9507\n",
      "Epoch 104/250\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.95 - 1s 10ms/step - loss: 0.2023 - accuracy: 0.9507\n",
      "Epoch 105/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1996 - accuracy: 0.9507\n",
      "Epoch 106/250\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.2007 - accuracy: 0.9507\n",
      "Epoch 107/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2004 - accuracy: 0.9507\n",
      "Epoch 108/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2014 - accuracy: 0.9507\n",
      "Epoch 109/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2028 - accuracy: 0.9507\n",
      "Epoch 110/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2041 - accuracy: 0.9507\n",
      "Epoch 111/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2006 - accuracy: 0.9507\n",
      "Epoch 112/250\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.2024 - accuracy: 0.9507\n",
      "Epoch 113/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1980 - accuracy: 0.9507\n",
      "Epoch 114/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1985 - accuracy: 0.9507\n",
      "Epoch 115/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1999 - accuracy: 0.9507\n",
      "Epoch 116/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2037 - accuracy: 0.9507\n",
      "Epoch 117/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1994 - accuracy: 0.9507\n",
      "Epoch 118/250\n",
      "72/72 [==============================] - 2s 21ms/step - loss: 0.2017 - accuracy: 0.9507\n",
      "Epoch 119/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1999 - accuracy: 0.9507\n",
      "Epoch 120/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1990 - accuracy: 0.9507\n",
      "Epoch 121/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2014 - accuracy: 0.9507\n",
      "Epoch 122/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2019 - accuracy: 0.9507\n",
      "Epoch 123/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.2002 - accuracy: 0.9507\n",
      "Epoch 124/250\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.2013 - accuracy: 0.9507\n",
      "Epoch 125/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2029 - accuracy: 0.9507\n",
      "Epoch 126/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2008 - accuracy: 0.9507\n",
      "Epoch 127/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1976 - accuracy: 0.9507\n",
      "Epoch 128/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2035 - accuracy: 0.9507\n",
      "Epoch 129/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1994 - accuracy: 0.9507\n",
      "Epoch 130/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2004 - accuracy: 0.9507\n",
      "Epoch 131/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1997 - accuracy: 0.9507\n",
      "Epoch 132/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2021 - accuracy: 0.9507\n",
      "Epoch 133/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2008 - accuracy: 0.9507\n",
      "Epoch 134/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1990 - accuracy: 0.9507\n",
      "Epoch 135/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2025 - accuracy: 0.9507\n",
      "Epoch 136/250\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1994 - accuracy: 0.9507\n",
      "Epoch 137/250\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.1966 - accuracy: 0.9507\n",
      "Epoch 138/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1979 - accuracy: 0.9507\n",
      "Epoch 139/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1964 - accuracy: 0.9507\n",
      "Epoch 140/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1972 - accuracy: 0.9507\n",
      "Epoch 141/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1989 - accuracy: 0.9507\n",
      "Epoch 142/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1965 - accuracy: 0.9507\n",
      "Epoch 143/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2015 - accuracy: 0.9507\n",
      "Epoch 144/250\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.2023 - accuracy: 0.9507\n",
      "Epoch 145/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1966 - accuracy: 0.9507\n",
      "Epoch 146/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1997 - accuracy: 0.9507\n",
      "Epoch 147/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1968 - accuracy: 0.9507\n",
      "Epoch 148/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1984 - accuracy: 0.9507\n",
      "Epoch 149/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1988 - accuracy: 0.9507\n",
      "Epoch 150/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1977 - accuracy: 0.9507\n",
      "Epoch 151/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1976 - accuracy: 0.9507\n",
      "Epoch 152/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1982 - accuracy: 0.9507\n",
      "Epoch 153/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2007 - accuracy: 0.9507\n",
      "Epoch 154/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1999 - accuracy: 0.9507\n",
      "Epoch 155/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2018 - accuracy: 0.9507\n",
      "Epoch 156/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1967 - accuracy: 0.9507\n",
      "Epoch 157/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1995 - accuracy: 0.9507\n",
      "Epoch 158/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1978 - accuracy: 0.9507\n",
      "Epoch 159/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1987 - accuracy: 0.9507\n",
      "Epoch 160/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1941 - accuracy: 0.9507\n",
      "Epoch 161/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1988 - accuracy: 0.9507\n",
      "Epoch 162/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1969 - accuracy: 0.9507\n",
      "Epoch 163/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1974 - accuracy: 0.9507\n",
      "Epoch 164/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1994 - accuracy: 0.9507\n",
      "Epoch 165/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1972 - accuracy: 0.9507\n",
      "Epoch 166/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1994 - accuracy: 0.9507\n",
      "Epoch 167/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1984 - accuracy: 0.9507\n",
      "Epoch 168/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2000 - accuracy: 0.9507\n",
      "Epoch 169/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1973 - accuracy: 0.9507\n",
      "Epoch 170/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1979 - accuracy: 0.9507\n",
      "Epoch 171/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1961 - accuracy: 0.9507\n",
      "Epoch 172/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.2010 - accuracy: 0.9507\n",
      "Epoch 173/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1981 - accuracy: 0.9507\n",
      "Epoch 174/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1961 - accuracy: 0.9507\n",
      "Epoch 175/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1964 - accuracy: 0.9507\n",
      "Epoch 176/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1973 - accuracy: 0.9507\n",
      "Epoch 177/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1987 - accuracy: 0.9507\n",
      "Epoch 178/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1974 - accuracy: 0.9507\n",
      "Epoch 179/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1981 - accuracy: 0.9507\n",
      "Epoch 180/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1972 - accuracy: 0.9507\n",
      "Epoch 181/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1973 - accuracy: 0.9507\n",
      "Epoch 182/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1994 - accuracy: 0.9507\n",
      "Epoch 183/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1961 - accuracy: 0.9507 0s - loss: 0\n",
      "Epoch 184/250\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.1969 - accuracy: 0.9507\n",
      "Epoch 185/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1974 - accuracy: 0.9507\n",
      "Epoch 186/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1988 - accuracy: 0.9507\n",
      "Epoch 187/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1961 - accuracy: 0.9507\n",
      "Epoch 188/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1965 - accuracy: 0.9507\n",
      "Epoch 189/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1963 - accuracy: 0.9507\n",
      "Epoch 190/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1977 - accuracy: 0.9507\n",
      "Epoch 191/250\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.1968 - accuracy: 0.9507\n",
      "Epoch 192/250\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.95 - 1s 10ms/step - loss: 0.1941 - accuracy: 0.9507\n",
      "Epoch 193/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1976 - accuracy: 0.9507\n",
      "Epoch 194/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1970 - accuracy: 0.9507\n",
      "Epoch 195/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1994 - accuracy: 0.9507\n",
      "Epoch 196/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1962 - accuracy: 0.9507\n",
      "Epoch 197/250\n",
      "72/72 [==============================] - 1s 17ms/step - loss: 0.1970 - accuracy: 0.9507\n",
      "Epoch 198/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1962 - accuracy: 0.9507\n",
      "Epoch 199/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1963 - accuracy: 0.9507\n",
      "Epoch 200/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1965 - accuracy: 0.9507\n",
      "Epoch 201/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1967 - accuracy: 0.9507\n",
      "Epoch 202/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1966 - accuracy: 0.9507\n",
      "Epoch 203/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1961 - accuracy: 0.9507 0s - loss: 0.1958 \n",
      "Epoch 204/250\n",
      "72/72 [==============================] - 1s 18ms/step - loss: 0.1993 - accuracy: 0.9507\n",
      "Epoch 205/250\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1974 - accuracy: 0.9507\n",
      "Epoch 206/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.1972 - accuracy: 0.9507\n",
      "Epoch 207/250\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1971 - accuracy: 0.9507\n",
      "Epoch 208/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.1980 - accuracy: 0.9507\n",
      "Epoch 209/250\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.1971 - accuracy: 0.9507\n",
      "Epoch 210/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1978 - accuracy: 0.9507\n",
      "Epoch 211/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1975 - accuracy: 0.9507\n",
      "Epoch 212/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1954 - accuracy: 0.9507\n",
      "Epoch 213/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1974 - accuracy: 0.9507\n",
      "Epoch 214/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1972 - accuracy: 0.9507\n",
      "Epoch 215/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1967 - accuracy: 0.9507\n",
      "Epoch 216/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1993 - accuracy: 0.9507\n",
      "Epoch 217/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1970 - accuracy: 0.9507\n",
      "Epoch 218/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1969 - accuracy: 0.9507\n",
      "Epoch 219/250\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.95 - 1s 10ms/step - loss: 0.1959 - accuracy: 0.9507\n",
      "Epoch 220/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1971 - accuracy: 0.9507\n",
      "Epoch 221/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1972 - accuracy: 0.9507\n",
      "Epoch 222/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1969 - accuracy: 0.9507\n",
      "Epoch 223/250\n",
      "72/72 [==============================] - 1s 19ms/step - loss: 0.1959 - accuracy: 0.9507\n",
      "Epoch 224/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1993 - accuracy: 0.9507\n",
      "Epoch 225/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1955 - accuracy: 0.9507\n",
      "Epoch 226/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1953 - accuracy: 0.9507\n",
      "Epoch 227/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1967 - accuracy: 0.9507\n",
      "Epoch 228/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1969 - accuracy: 0.9507\n",
      "Epoch 229/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1956 - accuracy: 0.9507\n",
      "Epoch 230/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1969 - accuracy: 0.9507\n",
      "Epoch 231/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1979 - accuracy: 0.9507\n",
      "Epoch 232/250\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1973 - accuracy: 0.9507\n",
      "Epoch 233/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1972 - accuracy: 0.9507\n",
      "Epoch 234/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1955 - accuracy: 0.9507\n",
      "Epoch 235/250\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1961 - accuracy: 0.9507\n",
      "Epoch 236/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1968 - accuracy: 0.9507\n",
      "Epoch 237/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1961 - accuracy: 0.9507\n",
      "Epoch 238/250\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1954 - accuracy: 0.9507 0s - loss: 0.1818 \n",
      "Epoch 239/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1958 - accuracy: 0.9507\n",
      "Epoch 240/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1952 - accuracy: 0.9507\n",
      "Epoch 241/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1973 - accuracy: 0.9507\n",
      "Epoch 242/250\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.1987 - accuracy: 0.9507\n",
      "Epoch 243/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1982 - accuracy: 0.9507\n",
      "Epoch 244/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1976 - accuracy: 0.9507\n",
      "Epoch 245/250\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.1968 - accuracy: 0.9507\n",
      "Epoch 246/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1956 - accuracy: 0.9507\n",
      "Epoch 247/250\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1964 - accuracy: 0.9507\n",
      "Epoch 248/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1967 - accuracy: 0.9507\n",
      "Epoch 249/250\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.1966 - accuracy: 0.95 - 1s 16ms/step - loss: 0.1975 - accuracy: 0.9507\n",
      "Epoch 250/250\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1961 - accuracy: 0.9507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a34f4acc40>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2a353201370>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(model.history.history)\n",
    "df['loss'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2a3509b1160>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['accuracy'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 613us/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.evaluate(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "train_predictions = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.788551e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.479597e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.210483e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.805672e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.775569e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.471517e-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.563916e-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.425753e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.138180e-14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.305256e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.773939e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.638591e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.589897e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.663956e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.421521e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.439524e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.256354e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.453933e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.412212e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.604602e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.793787e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.628098e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.248602e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.737356e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.304096e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.288763e-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.274306e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.388524e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.573694e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.917800e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.546231e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.417807e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.649732e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.171517e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.781476e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.200968e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.207006e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.452917e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6.254900e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.410809e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5.866349e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.528725e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.741484e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.457521e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.644376e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.850112e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.511920e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.771724e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>6.350556e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Predicted  Actual\n",
       "0   4.788551e-02       0\n",
       "1   6.479597e-02       0\n",
       "2   5.210483e-02       0\n",
       "3   4.805672e-02       0\n",
       "4   4.775569e-02       0\n",
       "5   4.471517e-02       1\n",
       "6   4.563916e-02       1\n",
       "7   6.425753e-02       0\n",
       "8   4.138180e-14       0\n",
       "9   6.305256e-02       0\n",
       "10  4.773939e-02       0\n",
       "11  4.638591e-02       0\n",
       "12  0.000000e+00       0\n",
       "13  4.589897e-02       0\n",
       "14  4.663956e-02       0\n",
       "15  6.421521e-02       0\n",
       "16  4.439524e-02       0\n",
       "17  6.256354e-02       0\n",
       "18  4.453933e-02       0\n",
       "19  5.412212e-02       0\n",
       "20  4.604602e-02       0\n",
       "21  4.793787e-02       0\n",
       "22  4.628098e-02       0\n",
       "23  6.248602e-02       0\n",
       "24  4.737356e-02       0\n",
       "25  4.304096e-02       0\n",
       "26  4.288763e-02       1\n",
       "27  4.274306e-02       0\n",
       "28  6.388524e-02       0\n",
       "29  4.573694e-02       0\n",
       "30  4.917800e-02       0\n",
       "31  4.546231e-02       0\n",
       "32  4.417807e-02       0\n",
       "33  4.649732e-02       0\n",
       "34  5.171517e-02       0\n",
       "35  4.781476e-02       0\n",
       "36  4.200968e-02       0\n",
       "37  4.207006e-02       0\n",
       "38  4.452917e-02       0\n",
       "39  6.254900e-02       0\n",
       "40  4.410809e-02       0\n",
       "41  5.866349e-02       0\n",
       "42  4.528725e-02       0\n",
       "43  4.741484e-02       0\n",
       "44  4.457521e-02       0\n",
       "45  4.644376e-02       0\n",
       "46  5.850112e-02       0\n",
       "47  4.511920e-02       0\n",
       "48  4.771724e-02       0\n",
       "49  6.350556e-02       0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.DataFrame(test_predictions, columns=['Predicted'])\n",
    "eval_df['Actual'] = y_test.values\n",
    "eval_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
